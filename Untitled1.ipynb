{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1jxO3bvevO7Y8gwY0Dok0pCnUGvwRtn8g","timestamp":1742403576777}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# # 1. Upload dataset folder in Colab\n","# print(\"Please upload your 'xyzdataset' folder containing .txt files\")\n","# uploaded = files.upload()  # Upload the folder as a zip and extract manually if needed\n","\n","# If uploaded as zip, uncomment and run this:\n","\n","!unzip xyzdataset.zip -d xyzdataset\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkHirj-E0Rf-","executionInfo":{"status":"ok","timestamp":1742399635671,"user_tz":-330,"elapsed":415,"user":{"displayName":"Satyam kumar Singh","userId":"03953500951746694819"}},"outputId":"2b1e09c9-0368-4910-a5d2-1e9010ef397f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  xyzdataset.zip\n","  inflating: xyzdataset/train_snli.txt  \n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Read the dataset from xyz_dataset.txt\n","with open(\"/content/xyzdataset/train_snli.txt\", \"r\", encoding=\"utf-8\") as file:\n","    lines = [line.strip() for line in file.readlines() if line.strip()]\n","\n","# Extract labels and texts (split by tabs, label is last)\n","labels = []\n","texts = []\n","for line in lines:\n","    parts = line.split(\"\\t\")  # Split by tab\n","    if len(parts) >= 3:  # Ensure there are at least 3 parts (text1, text2, label)\n","        label = int(parts[-1])  # Last part is the label (0 or 1)\n","        text = \"\\t\".join(parts[:-1])  # Keep text segments with tab separator\n","        labels.append(label)\n","        texts.append(text)\n","    else:\n","        print(f\"Skipping malformed line: '{line}'\")\n","\n","# Split: 80% train (8000 lines), 20% test (2000 lines)\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",")\n","\n","# Recombine into original format (text1\\ttext2\\tlabel)\n","train_lines = [f\"{text}\\t{label}\" for text, label in zip(train_texts, train_labels)]\n","test_lines = [f\"{text}\\t{label}\" for text, label in zip(test_texts, test_labels)]\n","\n","# Save to files\n","with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(train_lines))\n","with open(\"test.txt\", \"w\", encoding=\"utf-8\") as f:\n","    f.write(\"\\n\".join(test_lines))\n","\n","# Verify sizes and distribution\n","print(f\"Training set: {len(train_lines)} lines (0s: {train_labels.count(0)}, 1s: {train_labels.count(1)})\")\n","print(f\"Test set: {len(test_lines)} lines (0s: {test_labels.count(0)}, 1s: {test_labels.count(1)})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dZgYYthgIB49","executionInfo":{"status":"ok","timestamp":1742399894049,"user_tz":-330,"elapsed":1440,"user":{"displayName":"Satyam kumar Singh","userId":"03953500951746694819"}},"outputId":"ca90083b-93f9-4c85-c779-58b7c7025fc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training set: 293898 lines (0s: 147173, 1s: 146725)\n","Test set: 73475 lines (0s: 36793, 1s: 36682)\n"]}]},{"cell_type":"code","source":["import os\n","import string\n","import re\n","import nltk\n","import numpy as np\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","import pickle\n","\n","# Download NLTK resources (run once)\n","nltk.download('punkt')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# 2. Function to read .txt files\n","def read_txt(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        return file.read()\n","\n","# 3. Load and split the dataset from xyz_dataset.txt\n","dataset_path = '/content/xyzdataset/train_snli.txt'  # Adjust path if needed\n","documents = []\n","labels = []\n","\n","# Read the single file and split into documents and labels\n","if os.path.exists(dataset_path):\n","    with open(dataset_path, 'r', encoding='utf-8') as file:\n","        lines = [line.strip() for line in file.readlines() if line.strip()]\n","\n","    for line in lines:\n","        parts = line.split('\\t')  # Split by tab\n","        if len(parts) >= 3:  # text1, text2, label\n","            text = \"\\t\".join(parts[:-1])  # Combine text1 and text2 with tab\n","            label = int(parts[-1])  # Last part is 0 or 1\n","            documents.append(text)\n","            labels.append(label)\n","else:\n","    raise FileNotFoundError(f\"{dataset_path} not found!\")\n","\n","# Split: 80% train (8000 lines), 20% test (2000 lines)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    documents, labels, test_size=0.2, random_state=42, stratify=labels\n",")\n","\n","# 4. Data Preprocessing Function\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = str(text).lower()\n","\n","    # Remove punctuation\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","\n","    # Remove numbers\n","    text = re.sub(r'\\d+', '', text)\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Join tokens back to text\n","    return ' '.join(tokens)\n","\n","# 5. Preprocess all documents\n","processed_docs_train = [preprocess_text(doc) for doc in X_train]\n","processed_docs_test = [preprocess_text(doc) for doc in X_test]\n","\n","# 7. Create TF-IDF Vectorizer and fit on training data\n","vectorizer = TfidfVectorizer()\n","tfidf_train = vectorizer.fit_transform(processed_docs_train)\n","tfidf_test = vectorizer.transform(processed_docs_test)\n","\n","# 8. Plagiarism Checker Class\n","class PlagiarismChecker:\n","    def __init__(self, vectorizer, tfidf_matrix, threshold=0.8):\n","        self.vectorizer = vectorizer\n","        self.tfidf_matrix = tfidf_matrix\n","        self.threshold = threshold\n","\n","    def check_plagiarism(self, text):\n","        # Preprocess input text\n","        processed_text = preprocess_text(text)\n","\n","        # Transform text to TF-IDF\n","        text_tfidf = self.vectorizer.transform([processed_text])\n","\n","        # Calculate cosine similarity with all documents\n","        similarities = cosine_similarity(text_tfidf, self.tfidf_matrix)[0]\n","\n","        # Find maximum similarity\n","        max_similarity = np.max(similarities)\n","\n","        # Return result\n","        return {\n","            'is_plagiarized': max_similarity >= self.threshold,\n","            'similarity_score': max_similarity,\n","            'most_similar_doc_index': np.argmax(similarities)\n","        }\n","\n","# 9. Training and Evaluation\n","checker = PlagiarismChecker(vectorizer, tfidf_train)\n","\n","# Test the model\n","y_pred = []\n","for doc in processed_docs_test:\n","    result = checker.check_plagiarism(doc)\n","    y_pred.append(1 if result['is_plagiarized'] else 0)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n","\n","# 10. Working Demo Function\n","def demo_plagiarism_checker(text):\n","    result = checker.check_plagiarism(text)\n","    print(\"\\nPlagiarism Check Results:\")\n","    print(f\"Input Text: {text}\")\n","    print(f\"Similarity Score: {result['similarity_score']:.4f}\")\n","    print(f\"Plagiarism Detected: {result['is_plagiarized']}\")\n","    print(f\"Most Similar Document Index: {result['most_similar_doc_index']}\")\n","    print(f\"Most Similar Document: {X_train[result['most_similar_doc_index']]}\")\n","\n","# 11. Test the demo with sample texts\n","print(\"\\n=== Demo Tests ===\")\n","test_texts = [\n","    \"This is a test document about machine learning\",\n","    \"Completely unique and original content\",\n","    \"Machine learning powers modern technology\"\n","]\n","\n","for test_text in test_texts:\n","    demo_plagiarism_checker(test_text)\n","\n","# 12. Dataset Statistics\n","print(\"\\n=== xyz_dataset Statistics ===\")\n","print(f\"Total documents: {len(documents)}\")\n","print(f\"Number of training documents: {len(X_train)}\")\n","print(f\"Number of test documents: {len(X_test)}\")\n","print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n","print(f\"Plagiarized documents: {sum(labels)}\")\n","print(f\"Original documents: {len(labels) - sum(labels)}\")\n","\n","# Optional: Save the model\n","with open('plagiarism_checker.pkl', 'wb') as f:\n","    pickle.dump({\n","        'vectorizer': vectorizer,\n","        'tfidf_matrix': tfidf_train,\n","        'threshold': checker.threshold\n","    }, f)\n","print(\"\\nModel saved as 'plagiarism_checker.pkl'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EP6p4rYbJIXe","outputId":"373195cb-ec5b-4e34-9f2c-72a84a8894e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]}]}